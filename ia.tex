\documentclass[twocolumn,titlepage,12pt]{article}

\usepackage[color=yellow]{todonotes}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}

% Avoid crushing TODO notes.
% I opened https://github.com/henrikmidtiby/todonotes/issues/29 about this problem.
\setlength{\marginparwidth}{2cm}

\bibliographystyle{plain}

\begin{document}
\title{Lambda Calculus and the Y Combinator:  Creating Functional Recursion in a Language Without It}
\author{Erik Boesen, Candidate gsg256}
\date{\parbox{\linewidth}{\centering%
  \today\endgraf\bigskip
  Ms. Jennifer Jayson \& Mr. William Snyder\endgraf\medskip
  IB High Level Math\endgraf\medskip
  George Mason High School}}
\maketitle
\begin{abstract}
We explain the basics of the lambda calculus, an alternate system of mathematical representation and logic created in the 1930s by Princeton mathematician Alonzo Church. We take advantage of this new intuition to explain the Y combinator, a simple yet powerful construct for simulating the process of recursion in a mathematical language lacking such a concept or even the ability to name functions.
\end{abstract}

\onecolumn
\tableofcontents
\newpage
\twocolumn

\section{Where did the Lambda Calculus come from?}
The lambda calculus (sometimes notated ``$\lambda$-calculus'') is a system of mathematical notation and logic developed for the study of the elementary properties of functions \cite{rojastutorial}.

\subsection{A brief history}
The notation and concept of the lambda calculus was first described in the 1930s by Alonzo Church \cite{church}.\footnote{Church later published a revision \cite{church2} of the system after other researchers criticized it as logically inconsistent \cite{logicallyinconsistent}. This revision brought the calculus to roughly its current form.} Despite lacking many attributes of standard maths, including basic operators and even a native concept of numbers, the system can be used to represent any computation traditionally possible. Even complex computations such as traditional differential calculus \cite{differentiallc}\footnote{This being the calculus learned by millions of students each year in high school classrooms.} can be represented using the lambda calculus. So, it is equivalent in capability to a ``Turing machine''---a mathematical notion of a computer. Both Church and Alan Turing, who proposed the idea of Turing machines, sought to solve the \textit{Entscheidungsproblem}. Translating to ``decision problem'', this question was posed by logician David Hilbert in 1928, and asked whether an algorihm can be devised to take as input a first-order logical construction, and output a boolean (true or false, yes or no) stating whether that logical construction is universally valid \cite{hilbert}. Both Church and Turing proved that such an algorithm could not exist, but did so independently and in different countries. Church used the new construct of the lambda calculus to respond to the problem, while Turing used the competing---and comparatively simple---construct of the Turing machine.\footnote{Interestingly, despite their sometimes conflicting ideas on computation, Turing studied as a Ph. D. candidate under Church from 1936 to 1938 \cite{churchpapers}.} The joint negatory answer given by the two computer scientists to the \textit{Entscheidungsproblem} is encompassed under the famous ``Church-Turing thesis'' \cite{churchturingthesis}. Unlike Turing machines, the lambda calculus never uses internal state to make decisions: functions cannot store data, and the same inputs will always yield the same outputs.

More recently, the system and its logical process has gained new notoriety as the basis for new forms of computer programming. The lambda calculus is a ``universal programming language'' \cite{rojastutorial}, and can, on its own, represent and perform the computations of a Turing machine and vice versa, even without a computer to run it. The lambda calculus is the basis for functional programming languages, or those whose attributes include treating functions as data and using a declarative programming paradigm\footnote{Declarative programming refers to the style of programming or mathematical notation where logic, not ``control flow,'' of a program is defined \cite{declarativeprogadv}. Rather than specifying a sequence of instructions, a declarative programmer would specify what logic functions contain, not necessarily in a linear manner. In short, while non-declarative (``imperative'') programming involves determination of what a function should \textit{do}, declarative programming emphasizes what a function \textit{is}.} \cite{hudakevolution}. The topic of functional programming is beyond the scope of this paper. Interested readers, however, are invited to investigate the topic through papers including \cite{totalfp} and \cite{hudakevolution}.

\section{Lambda Notation}
\subsection{Comparisons to conventional notation}
The notation used to describe lambda calculus expressions seldom bears intentional resemblance to more traditional systems of mathematical notation. To describe, for instance, the identity function (in which a single parameter is given as input and is then returned without modification) in traditional notation, one would likely use some variation upon:
$$f(x)=x$$
To make the function anonymous---remove its name---in traditional notation, one could use a ``map'' expression \cite{intrographtheory}, which simply lists the parameters to a function and, following the map symbol $\mapsto$, gives the function's internal logic:
$$x\mapsto x$$
Functions in traditional mathematics can easily be translated to use anonymous mapping. For example, the function for finding the length of a three-dimensional vector
$$L(x,y,z)=\sqrt{x^2+y^2+z^2}$$
could become
$$(x,y,z) \mapsto \sqrt{x^2+y^2+z^2}$$

\subsection{The basics}
Understanding this syntax, the transition to lambda calculus notation is relatively intuitive. Let us return to our identity map $x\mapsto x$ from above. In the lambda calculus, an anonymous identity function would be notated thus:
$$\lambda x.x$$
In addition to variables, which, as in conventional mathematics, can be any symbol, only two operational symbols in the lambda calculus are defined. These are $\lambda$ and $.$ (some other symbols, such as parentheses, are used intuitively for grouping and other purposes). The former is used to represent a function definition, whereas the latter separates parameters from that function's internal logic. The sum of those two components is referred to as an ``abstraction,'' which is roughly a synonym for a function definition.

The lambda calculus also defines ``application,'' the passing of a  function to a parameter to a function \cite{horowitz}. Lambda calculus expressions associate left-to-right. So, lambda abstractions can be applied simply by parenthesizing the abstraction and appending a parameter afterward.

After one constructs a lambda expression, one can solve it through the process of ``reduction.'' The most prevalent form of reduction is the process of substituting a parameter into a lambda function. That process is referred to as $\beta$-reduction (beta-reduction) \cite{hudakintro}. The first is $\alpha$-conversion (alpha-conversion), in which one variable name is substituted for another to prevent naming collisions, as in $\lambda x.B[x] \to \lambda y.B[y]$, where $B$ represents the lambda body \cite{hudakintro}. \footnote{The need for $\alpha$-conversion may be mitigated through use of a De Bruijn index, which we will for the moment ignore \cite{debruijn}.} The final reduction operation, and the most complex of the three, is $\eta$-conversion, which states that for a bound variable x, $\lambda x.Bx \to B$. Stated more transparently, if there exists a lambda abstraction which does nothing beyond applying another abstraction to its single parameter, the outer abstraction can be removed in favor of a direct use of the wrapped one \cite{etared}.

As an example of the reduction process ($\beta$-reduction), to apply our identity function to some constant $C$, we would perform the following lambda computation:
$$(\lambda x.x)C$$
$$=x[x:=C]$$
$$=C$$
You will notice that we here make use of the syntax $x[x:=C]$. This is an idiom used during $\beta$-reduction to make clear the process of substitution being undertaken.

A final important distinction in the lambda calculus is that between bound and free variables. A bound variable is one which is a parameter to the lambda abstraction, whereas a free variable is any other variable. So, in the expression
$$\lambda \theta.\theta \lambda \xi$$
$\theta$ is a bound variable, and $\lambda \xi$ a free variable because while $\theta$ is a parameter to the abstraction given, $\lambda \xi$ is not \cite{stanfordlc}.

\subsection{Currying}
The lambda calculus' syntax for representation of multi-parameter functions is potentially surprising. Purely speaking, there is no notion of multi-parameter functions. Rather, the lambda calculus makes use of ``currying,'' a technique named after mathematician Haskell Curry. In this process, a function which would typically require multiple parameters transforms into a series of functions requiring only one parameter each. So,
$$\lambda xy.xy$$
would expand to
$$\lambda x.\lambda y.xy$$
These two expressions are equivalent, assuming that the impure syntax used in the first represents one function with three parameters $x$, $y$, and $z$. Let us see how this expression would be applied:
$$(\lambda x.\lambda y.\lambda z.x(yz))ABC$$
The outer abstraction would be applied first, and the single parameter it accepts, $x$, would be substituted into the body of the enclosed function. So, the above expression would first simplify as follows:
$$(\lambda y.\lambda z.A(yz)BC$$
$$(\lambda z.A(Bz))C$$
$$A(BC)$$

\section{Illustration: Defining Exponentiation}
The important thing to note about the aforementioned syntax is that in pure lambda calculus, there are no other symbols or syntatical constructions used. This leads to some occasionally verbose definitions for otherwise simple concepts, which must be redefined before they can be used. One must go through a frankly absurd number of operations in order to perform the simplest of mathematical processes. To illustrate, let us walk through the process of defining exponentiation. To any remotely experienced mathematician the operation is doubtless well known. In the lambda calculus, however, we must do a great deal of work to define this capability.

First, we must define what numbers are.\footnote{Yes, seriously.} An easy way to use numbers in the lambda calculus is through ``Church numerals.'' Church numerals are actually themselves lambda abstractions, which take in a function and compose it $n$ times. Church numerals are notated with an underline \cite{cornelllc}.
$$\underline{0}=\lambda f.\lambda x.x$$
$$\underline{1}=\lambda f.\lambda x.fx$$
$$\underline{2}=\lambda f.\lambda x.f(fx)$$
$$\underline{3}=\lambda f.\lambda x.f(f(fx))$$
$$\underline{4}=\lambda f.\lambda x.f(f(f(fx)))$$
$$...$$
The first step, having now invented numbers, towards exponentiation is the ``successor'' function. This lambda expression takes a function $f$ as input, then composes it so as to return, for input $n$, $n+1$.
$$\lambda n.\lambda f.\lambda x.f((n f) x)$$

Building upon the successor function, we can define addition as follows:
$$\lambda m.\lambda n.\lambda fx.(m f) ((n f) x)$$
Put simply, we execute succession $n$ times to add $n$ to the numeral $m$.

Building off this definition of addition, we can define multiplication as simply repeated addition. We'll need to use $\alpha$-conversion, renaming variables in the aforementioned addition function:
%$$\lambda m.\lambda n.\lambda fx.(m f) ((n f) x) \to \lambda m.\lambda n.\lambda fx.(m f) ((n f) x)$$
$$\lambda a.\lambda b.a ((\lambda m.\lambda n.\lambda fx.(m f) ((n f) x)) b) \underline{0}$$
This expression is clearly convoluted and difficult to interpret. This is a key flaw within the pure lambda calculus. Without the ability to name abstractions, simple expressions quickly become lengthy and inconvenient to deal with.\footnote{It should be noted that some sources, including \cite{cornellc}\cite{rojastutorial} and others, as well as in most functional programming langauges based upon the lambda calculus, mitigate this issue by naming lambda expressions and substituting in the expressions when solving. So, rather than the convoluted expression $\lambda a.\lambda b.a ((\lambda m.\lambda n.\lambda fx.(m f) ((n f) x)) b) \underline{0}$, one could define $ADD=\lambda m.\lambda n.\lambda fx.(m f) ((n f) x)$ and then write multiplication as $\lambda a.\lambda b.(ADD b) \underline{0}$. However, this is not pure lambda calculus.}

The relationship between addition and multiplication is roughly the same as that between multiplication and exponentiation. We simply need to repeat multiplication:
$$\lambda x.\lambda e.x ((\lambda a.\lambda b.a ((\lambda m.\lambda n.\lambda fx.(m f) ((n f) x)) b) \underline{0}) e) \underline{1}$$

In mathematical literature, occasionally conventional mathematical syntax will be mixed into lambda expressions. Strictly speaking, pure lambda calculus cannot use even the most basic of operators like $+$, $-$, $\times$, $\div$, exponents/roots, etc. However, for legibility and terseness, these symbols are often used. Reasonably so; it is far easier to understand this equivalent of the three-dimensional vector length formula:
$$\lambda x.\lambda y.\lambda z.\sqrt{x^2+y^2+z^2}$$
Conveying this operation as a pure lambda expression would require a prohibitively large quantity of space and writing.

\section{Implementing recursion in the lambda calculus}
\subsection{What is recursion?}
As has been previously alluded to, one of the most bizarre characteristics of the lambda calculus is its lack of natural support for naming functions. A consequence of this deficit is that the process of ``recursion'' is not as easy as one might be accustomed to.

Recursion is a technique used frequently within the field of computer science, but it finds much use within mathematical operations as well. A classic example of recursion is a redefinition of the factorial operator:
\[
factorial(x)=
\begin{cases}
    x\times factorial(x-1) & x>0 \\
    1 & x=0
\end{cases}
\]
This function may appear initially confusing. An illustration may help. Let us take the factorial of the integer $4$. First, we begin with our initial call to the function:
$$factorial(4)$$
As $4>1$, the first case is met, so:
$$factorial(4)=4\times factorial(3)$$
From here, we can clearly see how the expression will expand.
$$factorial(4)$$
$$=4\times factorial(3)$$
$$=4\times 3\times factorial(2)$$
$$=4\times 3\times 2\times 1\times factorial(0)$$
Since evaluating $factorial(0)$ uses the special ``base case'' of $factorial$, the expression will evaluate to:
$$4\times 3\times 2\times 1\times 1=24$$

Knowing, however, that the lambda calculus does not allow us to name abstractions, we must use an alternative construction known as the ``Y combinator'' to achieve our end in defining recursion.

\subsection{The Y Combinator}
Since lambda expressions are anonymous, no strategy for programatic recursion using the notation is immediately evident. However, it is actually possible to create recursion even without naming any functions. To do so, we must implement the Y combinator \cite{ycombmedium}. The Y combinator originated in the lambda calculus as a useful tool for simulating recursion even in a calculus lacking names or otherwise not allowing for recursive computation. This technique was discovered by mathematician Haskell Curry, whom you may remember as the namesake of the process of currying \cite{compphileyc}.

To understand how the Y combinator works, we need to recall how lambda expressions associate. Lambda expressions are ``left-associative,'' meaning that the leftmost abstraction in an application will be evaluated first, and expressions to the right of one expression will be used as parameters to that on the left. \textit{Any} expression can be substituted as a parameter to a function. So:
$$(\lambda x.xx)\lambda x.x$$
$$=(\lambda x.x)(\lambda x.x)$$
$$=\lambda x.x$$

The Y combinator may be expressed as follows:
$$\lambda f.(\lambda x.f(x x))(\lambda x.f(x x))$$
To demonstrate how the Y combinator creates recursion, take the lambda abstraction $\lambda \omega.\omega$ (which, through $\alpha$-conversion, is equivalent to our identity function $\lambda x.x$ from earlier).

We start by applying the Y combinator, as above notated, to our abstraction.
$$(\lambda f.(\lambda x.f(x x))(\lambda x.f(x x)))(\lambda \omega.\omega)$$
In this instance, the Y combinator $(\lambda f.(\lambda x.f(x x))(\lambda x.f(x x)))$ accepts a single parameter of our lambda abstraction $\lambda \omega.\omega$ as parameter $f$. We can use $\beta$-reduction to reduce the expression by substituting $\lambda \omega.\omega$ for $f$ where it occurs within the Y combinator.
$$(\lambda x.f(x x))(\lambda x.f(x x))[f:=(\lambda \omega.\omega)]$$
$$(\lambda x.(\lambda \omega.\omega)(x x))(\lambda x.(\lambda \omega.\omega)(x x))$$
At this point we need to substitute the second outer lambda abstraction (not our identity) as the parameter $x$ to the first.\footnote{It is not necessarily to perform $\alpha$-conversion in this instance because the substitution of our new $x$ replaces all instances of that variable in the expression being applied. Therefore, there is no naming collision.}
$$(\lambda \omega.\omega)(x x)[x:=(\lambda x.(\lambda \omega.\omega)(x x))]$$
$$(\lambda \omega.\omega)((\lambda x.(\lambda \omega.\omega)(x x))(\lambda x.(\lambda \omega.\omega)(x x)))$$
At this point, you will notice that we are left roughly where we were before our second $\beta$-reduction, the only difference being that our reductions have left us with a copy of our identity abstraction, which we passed into the Y combinator as parameter $f$.

Through this process, the Y combinator is able to achieve recursion in a the lambda calculus---no naming required.

To further understand the Y combinator, a fascinating exercise for the reader would be to reimplement the $factorial$ function previously described. Doing so requires implementation of boolean values (true and false), conditional operators, and subtraction. These topics are described extensively in a variety of sources, including \cite{stanfordlc}\cite{cornelllc}.

\section{Conclusion}
Through this paper we reviewed the basics of the lambda calculus, a new system of mathematical notation and logic. To summarize what we covered:
\begin{itemize}
  \item{The lambda calculus includes three syntactic rules for constructing ``lambda expressions.'' We can define variables, use ``abstractions'' to define lambda functions, and use ``application'' to apply those functions to parameters.}
  \item{We can solve lambda expressions through reduction operations, especially $\beta$-reduction, or substitution of parameters into the body of a lambda expression.}
  \item{We can use these simple rules to define any mathematical operation which can be run by a computer.}
  \item{We can implement recursion through a lambda expression known as the Y combinator.}
\end{itemize}
There are quite a few other fascinating techniques encompassed within the area of lambda calculus which are worthy of study. We did not have time to touch on the fascinating topic of boolean logic, which is a fascinating concept crucial in the construction of higher-level abstractions around the lambda calculus. Nor did we discuss the formulation of a type system, a topic crucial to using lambda calculus in the context of functional programming.

\section{Reflection}
I first chose to research the lambda calculus due to my existing interest in the field of computer science. I've had past experience with the functional programming paradigm, the study of which was a truly transformative experience. Conventional programming---with languages like C, Java, Python, etc.---is structured around traditional mathematical structures. When I learned about functional programming, I occasionally heard references to the lambda calculus, an apparently mysterious and confusing alternate form for mathematical syntax and logic.
Through this Internal Assessment, I had an excellent opportunity to dive into the new and fascinating notion of this calculus, and the incredible idea that any computation can be represented using only two predefined symbols.

\bibliography{research}
\end{document}
